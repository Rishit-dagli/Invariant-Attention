{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 03:50:37.547872: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-06 03:50:40.481650: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-06 03:50:40.481683: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-06 03:50:47.614196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-06 03:50:47.614403: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-06 03:50:47.614417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def max_neg_value(t):\n",
    "    return -tf.experimental.numpy.finfo(t.dtype).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantPointAttention(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        scalar_key_dim = 16,\n",
    "        scalar_value_dim = 16,\n",
    "        point_key_dim = 4,\n",
    "        point_value_dim = 4,\n",
    "        pairwise_repr_dim = None,\n",
    "        require_pairwise_repr = True,\n",
    "        eps = 1e-8,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(InvariantPointAttention, self).__init__(**kwargs)\n",
    "\n",
    "        self.eps = eps\n",
    "        self.heads = heads\n",
    "        self.require_pairwise_repr = require_pairwise_repr\n",
    "\n",
    "        # num attention contributions\n",
    "\n",
    "        num_attn_logits = 3 if require_pairwise_repr else 2\n",
    "\n",
    "        # qkv projection for scalar attention (normal)\n",
    "\n",
    "        self.scalar_attn_logits_scale = (num_attn_logits * scalar_key_dim) ** -0.5\n",
    "\n",
    "        self.to_scalar_k = tf.keras.layers.Dense(scalar_key_dim * heads, use_bias = False)\n",
    "        self.to_scalar_k = tf.keras.layers.Dense(scalar_key_dim * heads, use_bias = False)\n",
    "        self.to_scalar_v = tf.keras.layers.Dense(scalar_value_dim * heads, use_bias = False)\n",
    "\n",
    "        point_weight_init_value = tf.math.log(tf.math.exp(tf.ones((heads,))) - 1.)\n",
    "        self.point_weights = tf.Variable(point_weight_init_value)\n",
    "\n",
    "        self.point_attn_logits_scale = ((num_attn_logits * point_key_dim) * (9 / 2)) ** -0.5\n",
    "\n",
    "        self.to_point_q = tf.keras.layers.Dense(point_key_dim * heads * 3, use_bias = False)\n",
    "        self.to_point_k = tf.keras.layers.Dense(point_key_dim * heads * 3, use_bias = False)\n",
    "        self.to_point_v = tf.keras.layers.Dense(point_value_dim * heads * 3, use_bias = False)\n",
    "\n",
    "        pairwise_repr_dim = default(pairwise_repr_dim, dim) if require_pairwise_repr else 0\n",
    "\n",
    "        if require_pairwise_repr:\n",
    "            self.pairwise_attn_logits_scale = num_attn_logits ** -0.5\n",
    "\n",
    "            self.to_pairwise_attn_bias = tf.keras.Sequential(\n",
    "                tf.keras.layers.Dense(heads),\n",
    "                Rearrange('b ... h -> (b h) ...')\n",
    "            )\n",
    "        \n",
    "        self.to_out = tf.keras.layers.Dense(dim)\n",
    "    \n",
    "    def call(\n",
    "        self,\n",
    "        single_repr,\n",
    "        pairwise_repr = None,\n",
    "        *,\n",
    "        rotations,\n",
    "        translations,\n",
    "        mask = None\n",
    "    ):\n",
    "        x, b, h, eps, require_pairwise_repr = single_repr, single_repr.shape[0], self.heads, self.eps, self.require_pairwise_repr\n",
    "        assert not (require_pairwise_repr and not exists(pairwise_repr)), 'pairwise representation must be given as second argument'\n",
    "\n",
    "        q_scalar, k_scalar, v_scalar = self.to_scalar_q(x), self.to_scalar_k(x), self.to_scalar_v(x)\n",
    "\n",
    "        q_point, k_point, v_point = self.to_point_q(x), self.to_point_k(x), self.to_point_v(x)\n",
    "\n",
    "        q_scalar, k_scalar, v_scalar = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q_scalar, k_scalar, v_scalar))\n",
    "        q_point, k_point, v_point = map(lambda t: rearrange(t, 'b n (h d c) -> (b h) n d c', h = h, c = 3), (q_point, k_point, v_point))\n",
    "\n",
    "        rotations = repeat(rotations, 'b n r1 r2 -> (b h) n r1 r2', h = h)\n",
    "        translations = repeat(translations, 'b n c -> (b h) n () c', h = h)\n",
    "\n",
    "        q_point = tf.einsum('b n d c, b n c r -> b n d r', q_point, rotations) + translations\n",
    "        k_point = tf.einsum('b n d c, b n c r -> b n d r', k_point, rotations) + translations\n",
    "        v_point = tf.einsum('b n d c, b n c r -> b n d r', v_point, rotations) + translations\n",
    "\n",
    "        attn_logits_scalar = tf.einsum('b i d, b j d -> b i j', q_scalar, k_scalar) * self.scalar_attn_logits_scale\n",
    "\n",
    "        if require_pairwise_repr:\n",
    "            attn_logits_pairwise = self.to_pairwise_attn_bias(pairwise_repr) * self.pairwise_attn_logits_scale\n",
    "        \n",
    "        point_qk_diff = rearrange(q_point, 'b i d c -> b i () d c') - rearrange(k_point, 'b j d c -> b () j d c')\n",
    "        point_dist = (point_qk_diff ** 2).sum(dim = (-1, -2))\n",
    "\n",
    "        point_weights = tf.math.softplus(self.point_weights)\n",
    "        point_weights = repeat(point_weights, 'h -> (b h) () ()', b = b)\n",
    "\n",
    "        attn_logits_points = -0.5 * (point_dist * point_weights * self.point_attn_logits_scale)\n",
    "\n",
    "        attn_logits = attn_logits_scalar + attn_logits_points\n",
    "\n",
    "        if require_pairwise_repr:\n",
    "            attn_logits = attn_logits + attn_logits_pairwise\n",
    "        \n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b i -> b i ()') * rearrange(mask, 'b j -> b () j')\n",
    "            mask = repeat(mask, 'b i j -> (b h) i j', h = h)\n",
    "            mask_value = max_neg_value(attn_logits)\n",
    "            attn_logits = tf.where(mask, mask_value, attn_logits)\n",
    "        \n",
    "        attn = tf.nn.softmax(attn_logits, axis = -1)\n",
    "\n",
    "        results_scalar = tf.einsum('b i j, b j d -> b i d', attn, v_scalar)\n",
    "        attn_with_heads = rearrange(attn, '(b h) i j -> b h i j', h = h)\n",
    "\n",
    "        if require_pairwise_repr:\n",
    "            results_pairwise = tf.einsum('b h i j, b i j d -> b h i d', attn_with_heads, pairwise_repr)\n",
    "\n",
    "        results_points = tf.einsum('b i j, b j d c -> b i d c', attn, v_point)\n",
    "\n",
    "        results_points = tf.einsum('b n d c, b n c r -> b n d r', results_points - translations, rotations.transpose(-1, -2))\n",
    "        results_points_norm = tf.math.sqrt(tf.math.square(results_points).sum(axis = -1) + eps)\n",
    "\n",
    "        results_scalar = rearrange(results_scalar, '(b h) n d -> b n (h d)', h = h)\n",
    "        results_points = rearrange(results_points, '(b h) n d c -> b n (h d c)', h = h)\n",
    "        results_points_norm = rearrange(results_points_norm, '(b h) n d -> b n (h d)', h = h)\n",
    "\n",
    "        results = (results_scalar, results_points, results_points_norm)\n",
    "\n",
    "        if require_pairwise_repr:\n",
    "            results_pairwise = rearrange(results_pairwise, 'b h n d -> b n (h d)', h = h)\n",
    "            results = (*results, results_pairwise)\n",
    "        \n",
    "        results = tf.concat(results, axis = -1)\n",
    "        return self.to_out(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, mult = 1., num_layers = 2, activation = tf.keras.layers.ReLU, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.mult = mult\n",
    "        self.num_layers = num_layers\n",
    "        self.activation = activation\n",
    "        dim_hidden = dim * mult\n",
    "\n",
    "        self.layers = []\n",
    "        for ind in range(num_layers):\n",
    "            is_first = ind == 0\n",
    "            is_last  = ind == (num_layers - 1)\n",
    "            dim_in   = dim if is_first else dim_hidden\n",
    "            dim_out  = dim if is_last else dim_hidden\n",
    "\n",
    "            self.layers.append(tf.keras.layers.Dense(dim_out))\n",
    "\n",
    "            if is_last:\n",
    "                continue\n",
    "\n",
    "            self.layers.append(activation())\n",
    "        self.layers = tf.keras.Sequential(self.layers)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.layers(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 04:16:17.831806: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-06 04:16:17.839674: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-06 04:16:17.850602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-c2e541): /proc/driver/nvidia/version does not exist\n",
      "2023-01-06 04:16:17.958452: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-06 04:16:19.168158: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n",
      "2023-01-06 04:16:19.252402: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n",
      "2023-01-06 04:16:19.264578: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer 'invariant_point_attention' (type InvariantPointAttention).\n\n'InvariantPointAttention' object has no attribute 'to_scalar_q'\n\nCall arguments received by layer 'invariant_point_attention' (type InvariantPointAttention):\n  • single_repr=tf.Tensor(shape=(1, 256, 64), dtype=float32)\n  • pairwise_repr=tf.Tensor(shape=(1, 256, 256, 64), dtype=float32)\n  • rotations=tf.Tensor(shape=(1, 256, 3, 3), dtype=float32)\n  • translations=tf.Tensor(shape=(1, 256, 3), dtype=float32)\n  • mask=tf.Tensor(shape=(1, 256), dtype=bool)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m rotations     \u001b[39m=\u001b[39m repeat(tf\u001b[39m.\u001b[39meye(\u001b[39m3\u001b[39m), \u001b[39m'\u001b[39m\u001b[39m... -> b n ...\u001b[39m\u001b[39m'\u001b[39m, b \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, n \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m) \u001b[39m# (batch x seq x rot1 x rot2) - example is identity\u001b[39;00m\n\u001b[1;32m     15\u001b[0m translations  \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m3\u001b[39m)) \u001b[39m# translation, also identity for example\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m attn_out \u001b[39m=\u001b[39m attn(\n\u001b[1;32m     18\u001b[0m     single_repr,\n\u001b[1;32m     19\u001b[0m     pairwise_repr,\n\u001b[1;32m     20\u001b[0m     rotations \u001b[39m=\u001b[39;49m rotations,\n\u001b[1;32m     21\u001b[0m     translations \u001b[39m=\u001b[39;49m translations,\n\u001b[1;32m     22\u001b[0m     mask \u001b[39m=\u001b[39;49m mask\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[3], line 66\u001b[0m, in \u001b[0;36mInvariantPointAttention.call\u001b[0;34m(self, single_repr, pairwise_repr, rotations, translations, mask)\u001b[0m\n\u001b[1;32m     63\u001b[0m x, b, h, eps, require_pairwise_repr \u001b[39m=\u001b[39m single_repr, single_repr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire_pairwise_repr\n\u001b[1;32m     64\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (require_pairwise_repr \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(pairwise_repr)), \u001b[39m'\u001b[39m\u001b[39mpairwise representation must be given as second argument\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 66\u001b[0m q_scalar, k_scalar, v_scalar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_scalar_q(x), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_scalar_k(x), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_scalar_v(x)\n\u001b[1;32m     68\u001b[0m q_point, k_point, v_point \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_point_q(x), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_point_k(x), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_point_v(x)\n\u001b[1;32m     70\u001b[0m q_scalar, k_scalar, v_scalar \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m t: rearrange(t, \u001b[39m'\u001b[39m\u001b[39mb n (h d) -> (b h) n d\u001b[39m\u001b[39m'\u001b[39m, h \u001b[39m=\u001b[39m h), (q_scalar, k_scalar, v_scalar))\n",
      "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer 'invariant_point_attention' (type InvariantPointAttention).\n\n'InvariantPointAttention' object has no attribute 'to_scalar_q'\n\nCall arguments received by layer 'invariant_point_attention' (type InvariantPointAttention):\n  • single_repr=tf.Tensor(shape=(1, 256, 64), dtype=float32)\n  • pairwise_repr=tf.Tensor(shape=(1, 256, 256, 64), dtype=float32)\n  • rotations=tf.Tensor(shape=(1, 256, 3, 3), dtype=float32)\n  • translations=tf.Tensor(shape=(1, 256, 3), dtype=float32)\n  • mask=tf.Tensor(shape=(1, 256), dtype=bool)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b\"\\x05\\xb1\\x17H\\xa5:TCe\\xa6\\xfc\\x06\\x86\\xde\\xce\\xb4\\xa3\\x8f\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\\x02\\x04\\x02\\x05\\x02\"]\n",
      "Bad pipe message: %s [b\"\\xc4\\xe3\\xad\\xbe\\xb5cA\\xd7\\xa1\\x10\\x7fW\\xb2 \\xf8\\x17\\x92e\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\", b'\\x0b\\xc0\\x01\\x00;\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e']\n",
      "Bad pipe message: %s [b\"yS\\xc2\\xd6\\xa9(\\\\\\r/\\xba\\xc5Q\\xf7\\xb8\\xcd<\\xa6\\x13\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\"]\n",
      "Bad pipe message: %s [b'njE\\xea\\xbc\\xda*\\x7f\\x14\\xfa\\x9f\\x9a$\\x96\\x03|\\xa4D\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01\\x15\\x03', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x92,\\x98\\x07K\"n\\xe0\\x8a\\xed\\xc1h<T\\xd2\\x81O\\x9e\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n']\n",
      "Bad pipe message: %s [b'>']\n",
      "Bad pipe message: %s [b'\\xc6b\\xfe%V=\\xfa\\x806Q\\xe5', b'\\xce\\x9f\\x0b\\x89\\xa5\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01\\x15\\x03\\x01\\x00\\x02\\x02F']\n",
      "Bad pipe message: %s [b'\\xe0w\\xa2%d\\xcc\\xfa\\x81^o\\xff\\x06aP#&\\xd5e\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98']\n",
      "Bad pipe message: %s [b'\\xb9\\xe5\\x00s\\xa2\\x94l\\xf7\\x15\\xde\\x1c\\xe5S\\x1c@\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1']\n",
      "Bad pipe message: %s [b'\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c']\n"
     ]
    }
   ],
   "source": [
    "attn = InvariantPointAttention(\n",
    "    dim = 64,                  # single (and pairwise) representation dimension\n",
    "    heads = 8,                 # number of attention heads\n",
    "    scalar_key_dim = 16,       # scalar query-key dimension\n",
    "    scalar_value_dim = 16,     # scalar value dimension\n",
    "    point_key_dim = 4,         # point query-key dimension\n",
    "    point_value_dim = 4        # point value dimension\n",
    ")\n",
    "\n",
    "single_repr   = tf.random.normal((1, 256, 64))      # (batch x seq x dim)\n",
    "pairwise_repr = tf.random.normal((1, 256, 256, 64)) # (batch x seq x seq x dim)\n",
    "mask          = tf.ones((1, 256), dtype = tf.bool) # # (batch x seq)\n",
    "\n",
    "rotations     = repeat(tf.eye(3), '... -> b n ...', b = 1, n = 256) # (batch x seq x rot1 x rot2) - example is identity\n",
    "translations  = tf.zeros((1, 256, 3)) # translation, also identity for example\n",
    "\n",
    "attn_out = attn(\n",
    "    single_repr,\n",
    "    pairwise_repr,\n",
    "    rotations = rotations,\n",
    "    translations = translations,\n",
    "    mask = mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Dec 19 2022, 20:24:16) [GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
