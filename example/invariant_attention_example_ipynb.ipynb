{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeplg+fKTvave+JmBjQ7R4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishit-dagli/Invariant-Attention/blob/main/example/invariant_attention_example_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invariant Attention Example\n",
        "\n",
        "This notebook shows the the process of using the `invariant-attention` Python package. Invariant Point Attention which was used in the structure module of Alphafold2 from the paper Highly accurate protein structure prediction with AlphaFold for coordinate refinement. Invariant Point Attention is a form of attention that acts on a set of frames and is invariant under global Euclidean transformations on said frames.\n",
        "\n",
        "If you find this useful please consider giving a ⭐ to the [repo](https://github.com/Rishit-dagli/Invariant-Attention/)."
      ],
      "metadata": {
        "id": "WykTMd5yVmhk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0_a36bROurV"
      },
      "outputs": [],
      "source": [
        "!pip install invariant-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "1grPvC2mV248"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "VaGvkEFzVmFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standalone IPA"
      ],
      "metadata": {
        "id": "YtQy-ei8V634"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ivariant_attention import InvariantPointAttention"
      ],
      "metadata": {
        "id": "yoWLQUx1WALx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn = InvariantPointAttention(\n",
        "    dim=64,  # single (and pairwise) representation dimension\n",
        "    heads=8,  # number of attention heads\n",
        "    scalar_key_dim=16,  # scalar query-key dimension\n",
        "    scalar_value_dim=16,  # scalar value dimension\n",
        "    point_key_dim=4,  # point query-key dimension\n",
        "    point_value_dim=4,  # point value dimension\n",
        ")\n",
        "\n",
        "single_repr = tf.random.normal((1, 256, 64))  # (batch x seq x dim)\n",
        "pairwise_repr = tf.random.normal((1, 256, 256, 64))  # (batch x seq x seq x dim)\n",
        "mask = tf.ones((1, 256), dtype=tf.bool)  # # (batch x seq)\n",
        "\n",
        "rotations = repeat(\n",
        "    tf.eye(3), \"... -> b n ...\", b=1, n=256\n",
        ")\n",
        "translations = tf.zeros((1, 256, 3))\n",
        "\n",
        "attn_out = attn(\n",
        "    single_repr,\n",
        "    pairwise_repr,\n",
        "    rotations=rotations,\n",
        "    translations=translations,\n",
        "    mask=mask,\n",
        ") # (1, 256, 64)"
      ],
      "metadata": {
        "id": "HIy826j6VR76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running an IPA Block"
      ],
      "metadata": {
        "id": "3DLJ2XXdWE-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from invariant_attention import IPABlock"
      ],
      "metadata": {
        "id": "xENUtRPyWG8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block = IPABlock(\n",
        "    dim=64,\n",
        "    heads=8,\n",
        "    scalar_key_dim=16,\n",
        "    scalar_value_dim=16,\n",
        "    point_key_dim=4,\n",
        "    point_value_dim=4,\n",
        ")\n",
        "\n",
        "seq = tf.random.normal((1, 256, 64))\n",
        "pairwise_repr = tf.random.normal((1, 256, 256, 64))\n",
        "mask = tf.ones((1, 256), dtype=tf.bool)\n",
        "\n",
        "rotations = repeat(tf.eye(3), \"... -> b n ...\", b=1, n=256)\n",
        "translations = tf.zeros((1, 256, 3))\n",
        "\n",
        "block_out = block(\n",
        "    seq,\n",
        "    pairwise_repr=pairwise_repr,\n",
        "    rotations=rotations,\n",
        "    translations=translations,\n",
        "    mask=mask,\n",
        ")\n",
        "\n",
        "updates = tf.keras.layers.Dense(6)(block_out)\n",
        "quaternion_update, translation_update = tf.split(\n",
        "    updates, num_or_size_splits=2, axis=-1\n",
        ")  # (1, 256, 3), (1, 256, 3)"
      ],
      "metadata": {
        "id": "4UYSVbfaWLYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running an IPATransformer"
      ],
      "metadata": {
        "id": "dVNgXs3PWNYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from invariant_attention import IPATransfomer"
      ],
      "metadata": {
        "id": "zX5tcDamWPnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = tf.random.normal((1, 256, 32))\n",
        "pairwise_repr = tf.random.normal((1, 256, 256, 32))\n",
        "mask = tf.ones((1, 256), dtype=tf.bool)\n",
        "translations = tf.zeros((1, 256, 3))\n",
        "\n",
        "model = IPATransformer(\n",
        "    dim=32,\n",
        "    depth=2,\n",
        "    num_tokens=None,\n",
        "    predict_points=False,\n",
        "    detach_rotations=True,\n",
        ")\n",
        "\n",
        "outputs = model(\n",
        "    single_repr=seq,\n",
        "    translations=translations,\n",
        "    quaternions=tf.random.normal((1, 256, 4)),\n",
        "    pairwise_repr=pairwise_repr,\n",
        "    mask=mask,\n",
        ") # (1, 256, 32), (1, 256, 3), (1, 256, 4)"
      ],
      "metadata": {
        "id": "EE4XUM7yWTk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}