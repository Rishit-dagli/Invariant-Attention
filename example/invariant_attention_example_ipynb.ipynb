{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYHXxs10fakzXR0QEvUabh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishit-dagli/Invariant-Attention/blob/main/example/invariant_attention_example_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invariant Attention Example\n",
        "\n",
        "This notebook shows the the process of using the `invariant-attention` Python package. Invariant Point Attention which was used in the structure module of Alphafold2 from the paper Highly accurate protein structure prediction with AlphaFold for coordinate refinement. Invariant Point Attention is a form of attention that acts on a set of frames and is invariant under global Euclidean transformations on said frames.\n",
        "\n",
        "If you find this useful please consider giving a ⭐ to the [repo](https://github.com/Rishit-dagli/Invariant-Attention/)."
      ],
      "metadata": {
        "id": "WykTMd5yVmhk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q0_a36bROurV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f95ab04-8cce-41b3-bb38-c08669f73bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting invariant-attention\n",
            "  Downloading Invariant_Attention-0.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tensorflow>=2.5.0 in /usr/local/lib/python3.8/dist-packages (from invariant-attention) (2.9.2)\n",
            "Collecting einops~=0.3.0\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (1.1.2)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (0.4.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (2.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (57.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (3.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (3.19.6)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (2.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (4.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (1.12)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (2.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (21.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (1.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (0.29.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (1.15.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.0->invariant-attention) (14.0.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.0->invariant-attention) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (2.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (1.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow>=2.5.0->invariant-attention) (3.0.9)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (5.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.5.0->invariant-attention) (3.2.2)\n",
            "Installing collected packages: einops, invariant-attention\n",
            "Successfully installed einops-0.3.2 invariant-attention-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install invariant-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "1grPvC2mV248"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from einops import repeat"
      ],
      "metadata": {
        "id": "VaGvkEFzVmFv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standalone IPA"
      ],
      "metadata": {
        "id": "YtQy-ei8V634"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from invariant_attention import InvariantPointAttention"
      ],
      "metadata": {
        "id": "yoWLQUx1WALx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn = InvariantPointAttention(\n",
        "    dim=64,  # single (and pairwise) representation dimension\n",
        "    heads=8,  # number of attention heads\n",
        "    scalar_key_dim=16,  # scalar query-key dimension\n",
        "    scalar_value_dim=16,  # scalar value dimension\n",
        "    point_key_dim=4,  # point query-key dimension\n",
        "    point_value_dim=4,  # point value dimension\n",
        ")\n",
        "\n",
        "single_repr = tf.random.normal((1, 256, 64))  # (batch x seq x dim)\n",
        "pairwise_repr = tf.random.normal((1, 256, 256, 64))  # (batch x seq x seq x dim)\n",
        "mask = tf.ones((1, 256), dtype=tf.bool)  # # (batch x seq)\n",
        "\n",
        "rotations = repeat(\n",
        "    tf.eye(3), \"... -> b n ...\", b=1, n=256\n",
        ")\n",
        "translations = tf.zeros((1, 256, 3))\n",
        "\n",
        "attn_out = attn(\n",
        "    single_repr,\n",
        "    pairwise_repr,\n",
        "    rotations=rotations,\n",
        "    translations=translations,\n",
        "    mask=mask,\n",
        ") # (1, 256, 64)"
      ],
      "metadata": {
        "id": "HIy826j6VR76"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running an IPA Block"
      ],
      "metadata": {
        "id": "3DLJ2XXdWE-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from invariant_attention import IPABlock"
      ],
      "metadata": {
        "id": "xENUtRPyWG8h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block = IPABlock(\n",
        "    dim=64,\n",
        "    heads=8,\n",
        "    scalar_key_dim=16,\n",
        "    scalar_value_dim=16,\n",
        "    point_key_dim=4,\n",
        "    point_value_dim=4,\n",
        ")\n",
        "\n",
        "seq = tf.random.normal((1, 256, 64))\n",
        "pairwise_repr = tf.random.normal((1, 256, 256, 64))\n",
        "mask = tf.ones((1, 256), dtype=tf.bool)\n",
        "\n",
        "rotations = repeat(tf.eye(3), \"... -> b n ...\", b=1, n=256)\n",
        "translations = tf.zeros((1, 256, 3))\n",
        "\n",
        "block_out = block(\n",
        "    seq,\n",
        "    pairwise_repr=pairwise_repr,\n",
        "    rotations=rotations,\n",
        "    translations=translations,\n",
        "    mask=mask,\n",
        ")\n",
        "\n",
        "updates = tf.keras.layers.Dense(6)(block_out)\n",
        "quaternion_update, translation_update = tf.split(\n",
        "    updates, num_or_size_splits=2, axis=-1\n",
        ")  # (1, 256, 3), (1, 256, 3)"
      ],
      "metadata": {
        "id": "4UYSVbfaWLYB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running an IPATransformer"
      ],
      "metadata": {
        "id": "dVNgXs3PWNYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from invariant_attention import IPATransformer"
      ],
      "metadata": {
        "id": "zX5tcDamWPnu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = tf.random.normal((1, 256, 32))\n",
        "pairwise_repr = tf.random.normal((1, 256, 256, 32))\n",
        "mask = tf.ones((1, 256), dtype=tf.bool)\n",
        "translations = tf.zeros((1, 256, 3))\n",
        "\n",
        "model = IPATransformer(\n",
        "    dim=32,\n",
        "    depth=2,\n",
        "    num_tokens=None,\n",
        "    predict_points=False,\n",
        "    detach_rotations=True,\n",
        ")\n",
        "\n",
        "outputs = model(\n",
        "    single_repr=seq,\n",
        "    translations=translations,\n",
        "    quaternions=tf.random.normal((1, 256, 4)),\n",
        "    pairwise_repr=pairwise_repr,\n",
        "    mask=mask,\n",
        ") # (1, 256, 32), (1, 256, 3), (1, 256, 4)"
      ],
      "metadata": {
        "id": "EE4XUM7yWTk4"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}